================================================================================
pblk: Physical Block Device Target
================================================================================

pblk is a LightNVM target implementing a full host-side Flash Translation Layer
(FTL). As a target, pblk presents itself as a make_request_fn base driver to the
block layer. In this particular case, as a block device driver, which exposes
the Open-Channel SSD storage device as a block storage to user space. In terms
of target functionality, pblk implements everything related to data placement,
garbage collection (GC) and recovery. In order to access the physical flash,
pblk makes use of LightNVM's generic media manager, which exposes a get/put
block provisioning interface.

By design, pblk supports pluggable mapping and GC algorithms so that a single
hardware device can be accommodated for specific workloads by simply making
changes in software to the desired module (e.g., to the data stripping strategy
in charge of mapping logical to physical addresses).

At the moment, pblk implements the following functionality:

  1. Sector-based host-side translation table management
  2. Write buffering for writing to physical flash pages
  3. Translation table recovery on power outage
  4. Retire bad flash blocks on write failure
  5. Simple cost-based garbage collector (GC)
  6. Sysfs integration
  7. I/O rate-limiting ??

1. Sector-based host-side translation table management
======================================================

 Mapping Strategy
 ----------------


 2. Write Buffer
 ===============
 pblk uses a ring write buffer of size N, where N is the power of two closest to
 the product of the number of active LUNS and the flash block size.

 Buffering facilitates sending writes to the controller in the size of and
 aligned to a given flash page size. A useful facility since constraints by
 controllers include writing aligned and at page granularity. Typically, flash
 pages are between 16KB and 32KB large, and the controller might introduce other
 abstractions that tie pages together (e.g., planes) and make the "apparent"
 page size relatively large (e.g., 64KB, 128KB).

 The flash media introduces additional constraints. Example, for a write to be
 guaranteed persistence on MLC flash, then it must be written in page pairs
 (lower and upper pages). Consequentially, the data in the lower flash page
 cannot be read (safely) until a write to the upper flash page occurs.
 Data must thus reside in the host until both writes are successful.
 We refer to the number of sectors that must reside in the write buffer before
 it is safe to read from the media as the buffers "grace area". We describe
 below how we implement this grace area through a dedicated pointer on the
 write buffer. This problem only aggravates with more complex media such as TLC
 or QLC.

 Buffering writes are thus a means to meet both controller- and media-specific
 constraints.

 Ring Write Buffer Design
 ------------------------
 Apart from the usual head (mem) and tail (subm) pointers, pblk's ring write
 buffer maintains a other pointers for different purposes. We describe each
 pointer separately:

   - Memory Pointer (mem): This is the head pointer. It points to the next
     writable entry on the buffer.
   - Submission Pointer (subm): This is the tail pointer. It points to the
     next buffered entry that is ready to be submitted to the media. Buffer
     space and count are calculated using mem and subm.
   - Sync Pointer (sync): It signals the last submitted entry that has
     completed; i.e., that has successfully been persisted to the media.
     It acts as a backpointer that guarantees that I/Os are completed in
     order. This is necessary to guarantee that flushes are completed
     correctly (See Documentation/block/writeback_cache_control.txt).
   - Flush Pointer (flush): It guarantees that flushes are respected
     by signaling the last entry associated to a REQ_FLUSH request.
     This pointer is taken into consideration by the write thread
     consuming the buffer.
   - Mapping Update Pointer (l2p_update): It guarantees that L2P
     lookups for entries that still reside on the write buffer
     cache will point to it, even if they have successfully been
     persisted to the media. By doing this we guarantee that all
     entries residing in the write buffer will be read from
     cache, thus preventing I/Os to be sent to the device to
     retrieve half-written flash pages. This way, we ensure that
     the whole buffer defines a grace area; the mapping table is
     only updated when the buffer head wraps up.

The write buffer is complemented with a write context buffer, which stores
metadata for each 4KB write. (XXX: Mapping strategy) As mentioned above, we
follow a late-map approach, where the actual mapping to flash pages is done when
the write buffer is being written to the media. This allows us to decouple the
mapping strategy from buffer writes, thus enabling mapping optimizations for
different workloads.

To minimize calculations, pblk's write buffer must be of a power-of-2 size...
also write that it is the closes one to the number of active LUNs in the target
X the size of a flash block.

          mem     l2p_update       sync                        subm
           |          |             |                            |
 -------------------------------------------------------------------------------
|          |          |   synced    |          submitted                       |
-------------------------------------------------------------------------------
| CCCCCCCC | DDDDDDD  | CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC|
 -------------------------------------------------------------------------------

C: L2P points to cache
D: L2P points to device


Write Path
----------
On a new write, the bio associated to it is converted into a write context
(pblk_w_ctx), where the logical block address (LBA), length and flush
requirements are stored.
The bio data is copied to the internal write buffer and completed if a flush is
not required (in case of a flush, the bio is completed after all data on the
write buffer has been persisted to the media). As data is stored on the write
buffer, the translation table is updated so that LBAs point to the buffer
entries data has been stored in.

A separate write thread consumes user data from the write buffer and forms new
bios that are then sent to the device. The mapping strategy described in Section
1 takes place here. This strategy can be modified, as long as controller
constrains are considered. If a flush is required, but there is not enough data
on the write buffer to fulfill these constrains, padding is performed. If the
write succeeds, the sync pointer will be updated on the completion path. When
the head pointer (mem) wraps up and overwrites old data, the l2p_update pointer
takes care of updating the translation table so that future lookups will point
to physical addresses on the device. As mentioned, this late mapping to device
addresses is the mechanism that we use to guarantee the buffer's grace area.

Note writes might fail. Look at Section 4 for an explanation on how to deal with
this case.

Read Path
---------
The read path is simpler than the write path. For each 4KB sector on the read
bio, a lookup is performed to find if data is cached on the write buffer or if
it is necessary to submit an I/O to retrieve it from the physical flash. It
might happen that only some sectors reside in cache. In this case, a new bio is
issue to retrieve sectors from the media, and the original bio is filled out
manually with them.


3. Translation table recovery
=============================
The L2P translation table is maintained entirely in host memory. The ratio
is approximately 1GB of L2P per 1TB of flash. Translation table recovery is
considered for two scenarios:

 (1) Graceful system shutdown (WIP)
 (2) Power failure

In the case of a graceful system shutdown, a L2P snapshot is stored on media.

When an L2P snapshot is not recoverable from media then pblk assumes that an
unexpected system shutdown, such as a power failure, has occurred.
In which case, pblk resorts to scanning the media and thereby reconstruct the
table. Two levels of redundancy are maintained to facilitate reconstruction:

  Firstly, for each 4KB sector, the LBA to physical page address (PPA) mapping
  is stored in the out-of-bound (OOB) area for that sector.

  Secondly, when a block is closed, the last page contains metadata on the block
  itself, including the LBA - PPA mapping list, block status and counters.

All recovery methods leverage functionality of the generic media manager... TODO

4. Write Recovery
=================

5. Garbage Collection
=====================

6. Sysfs integration
====================

7. I/O rate-limiting ??
====================
